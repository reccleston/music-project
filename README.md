# Top Tracks Playspace
### Team Members: Angeli, Sarah, Wasif, Ryan, Jini

![Songs]()

## Dataset we used:
- [Top Spotify Songs from 2010-2019 - Kaggle](https://www.kaggle.com/leonardopena/top-spotify-songs-from-20102019-by-year)
    * Context: The top songs BY YEAR in the world by spotify. This dataset has several variables about the songs and is based on Billboard
    * Content: There are the most popular songs in the world by year and 13 variables to be explored. Data were stracted from: http://organizeyourmusic.playlistmachinery.com/
    * Variables Measured:
        - Popularity/pop: how popular a song is. The higher the number, the more popular it is.
        - Speechiness/spch: how much spoken word is in the track.
        - Acoustic-ness/acous: how acoustic the song is.
        - Duration/dur: how long the track is (in seconds).
        - Valence/val: how positive the track is.
        - Liveness/live: how likely it is for the track to be a live recording.
        - Decibels/dB: how loud the track is.
        - Danceability/dnce: how easy it is to dance to the song.
        - Energy/nrgy: how energetic the song is.
        - Beats Per Minute/bpm: how many beats per minute, or, the trackâ€™s tempo.


## Summary and Motivation:
This webpage displays information from a Kaggle dataset on the top songs (according to Billboard rankings) worldwide on Spotify between 2010 and 2019.

We were inspired to work on this dataset because Spotify is a widely used streaming application, and drawing connections between different song qualities (here called "factors") seemed like an intriguing idea.
<!-- 
## Folders Directory
- [Wasif Folder](https://github.com/reccleston/music-project/tree/main/wasif) includes all relevant used for exploring, cleaning, and loading the data for each dataset separately 
    * [Resources Folder](https://github.com/RH-cmd/ETL-Case-Study/tree/main/Transformation%20Center/Resources) contains the original datasets
- [Entity Relationship Diagram](https://github.com/RH-cmd/ETL-Case-Study/blob/main/ERD.png) shows a visual of our table schema to load our data 
![ERD](https://github.com/RH-cmd/ETL-Case-Study/blob/main/ERD.png)
- [Merge and Upload Jupyter Notebook](https://github.com/RH-cmd/ETL-Case-Study/blob/main/merge_and_upload.ipynb) shows how we loaded the CSVs into Postgres using SQLalchemy. We used the jupyter notebook to test that our data loaded properly into the database. Then we used Postgres to merge and query our tables below.  
- [Table Schema](https://github.com/RH-cmd/ETL-Case-Study/blob/main/table_schema.sql) created the tables in Postgres and [Queries SQL](https://github.com/RH-cmd/ETL-Case-Study/blob/main/queries.sql) merged our datasets using specific parameters. -->



